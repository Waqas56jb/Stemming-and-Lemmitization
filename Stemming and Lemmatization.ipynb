{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "05d95f49-abbc-4416-9115-f4a733660bef",
   "metadata": {},
   "source": [
    "<h1 align=\"center\">Stemming in NLTK</h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "aee5b2b2-c333-4a32-a66d-f56eb575dfca",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem import PorterStemmer\n",
    "stemmer = PorterStemmer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c2771294-3a57-4b2f-b539-c206b6529294",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "eating | eat\n",
      "eats | eat\n",
      "eat | eat\n",
      "ate | ate\n",
      "adjustable | adjust\n",
      "rafting | raft\n",
      "ability | abil\n",
      "meeting | meet\n"
     ]
    }
   ],
   "source": [
    "words = [\"eating\", \"eats\", \"eat\", \"ate\", \"adjustable\", \"rafting\", \"ability\", \"meeting\"]\n",
    "\n",
    "for word in words:\n",
    "    print(word, \"|\", stemmer.stem(word))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ba76bbd-71a7-45f9-934f-8f4ba3e4ea5a",
   "metadata": {},
   "source": [
    "<h1 align=\"center\">Lemmatization in Spacy</h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "43383763-e8b3-42f9-89d9-52c6be07f90b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "eating  |  eat\n",
      "eats  |  eat\n",
      "eat  |  eat\n",
      "ate  |  eat\n",
      "adjustable  |  adjustable\n",
      "rafting  |  raft\n",
      "ability  |  ability\n",
      "meeting  |  meeting\n",
      "better  |  well\n"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "doc = nlp(\"Mando talked for 3 hours although talking isn't his thing\")\n",
    "doc = nlp(\"eating eats eat ate adjustable rafting ability meeting better\")\n",
    "for token in doc:\n",
    "    print(token, \" | \", token.lemma_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "8264801c-7f0e-4281-9dac-e0dcccb7aad8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['tok2vec', 'tagger', 'parser', 'attribute_ruler', 'lemmatizer', 'ner']"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nlp.pipe_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "71edec16-523c-4035-a2b8-2d27110993d0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bro | Brother\n",
      ", | ,\n",
      "you | you\n",
      "wanna | wanna\n",
      "go | go\n",
      "? | ?\n",
      "Brah | Brother\n",
      ", | ,\n",
      "do | do\n",
      "n't | not\n",
      "say | say\n",
      "no | no\n",
      "! | !\n",
      "I | I\n",
      "am | be\n",
      "exhausted | exhaust\n"
     ]
    }
   ],
   "source": [
    "ar = nlp.get_pipe('attribute_ruler')\n",
    "\n",
    "ar.add([[{\"TEXT\":\"Bro\"}],[{\"TEXT\":\"Brah\"}]],{\"LEMMA\":\"Brother\"})\n",
    "\n",
    "doc = nlp(\"Bro, you wanna go? Brah, don't say no! I am exhausted\")\n",
    "for token in doc:\n",
    "    print(token.text, \"|\", token.lemma_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "ec1d8bfb-be93-46b7-bb55-9da0d03206cd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Brah"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "doc[6]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "86ee0c0a-98c1-405e-ab4e-a450272f9ca6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Brother'"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "doc[6].lemma_\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "3797735a-076b-499e-bf6e-5557f6621a38",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4347558510128575363"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "doc[6].lemma"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0020ac93-4c4d-4ebd-8568-ef4e38fc5631",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "f742936f-dc6c-400e-af88-c1d8504cb461",
   "metadata": {},
   "source": [
    "<h1 align=\"center\">Part Of Speech POS Tagging</h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "406674b9-ca58-47f1-8f76-ee28c844c4fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "94c5c0c3-6a64-4972-b8c2-70a9dd39a46d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Elon  |  PROPN  |  proper noun\n",
      "flew  |  VERB  |  verb\n",
      "to  |  ADP  |  adposition\n",
      "mars  |  NOUN  |  noun\n",
      "yesterday  |  NOUN  |  noun\n",
      ".  |  PUNCT  |  punctuation\n",
      "He  |  PRON  |  pronoun\n",
      "carried  |  VERB  |  verb\n",
      "biryani  |  ADJ  |  adjective\n",
      "masala  |  NOUN  |  noun\n",
      "with  |  ADP  |  adposition\n",
      "him  |  PRON  |  pronoun\n"
     ]
    }
   ],
   "source": [
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "doc = nlp(\"Elon flew to mars yesterday. He carried biryani masala with him\")\n",
    "\n",
    "for token in doc:\n",
    "    print(token,\" | \", token.pos_, \" | \", spacy.explain(token.pos_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "72863001-5e01-4895-b6fe-e643e80e3a2b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wow  |  INTJ  |  interjection\n",
      "!  |  PUNCT  |  punctuation\n",
      "Dr.  |  PROPN  |  proper noun\n",
      "Strange  |  PROPN  |  proper noun\n",
      "made  |  VERB  |  verb\n",
      "265  |  NUM  |  numeral\n",
      "million  |  NUM  |  numeral\n",
      "$  |  NUM  |  numeral\n",
      "on  |  ADP  |  adposition\n",
      "the  |  DET  |  determiner\n",
      "very  |  ADV  |  adverb\n",
      "first  |  ADJ  |  adjective\n",
      "day  |  NOUN  |  noun\n"
     ]
    }
   ],
   "source": [
    "doc = nlp(\"Wow! Dr. Strange made 265 million $ on the very first day\")\n",
    "\n",
    "for token in doc:\n",
    "    print(token,\" | \", token.pos_, \" | \", spacy.explain(token.pos_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "7bd63568-3e6c-4ac6-8d11-385ca57eda64",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wow  |  INTJ  |  interjection  |  UH  |  interjection\n",
      "!  |  PUNCT  |  punctuation  |  .  |  punctuation mark, sentence closer\n",
      "Dr.  |  PROPN  |  proper noun  |  NNP  |  noun, proper singular\n",
      "Strange  |  PROPN  |  proper noun  |  NNP  |  noun, proper singular\n",
      "made  |  VERB  |  verb  |  VBD  |  verb, past tense\n",
      "265  |  NUM  |  numeral  |  CD  |  cardinal number\n",
      "million  |  NUM  |  numeral  |  CD  |  cardinal number\n",
      "$  |  NUM  |  numeral  |  CD  |  cardinal number\n",
      "on  |  ADP  |  adposition  |  IN  |  conjunction, subordinating or preposition\n",
      "the  |  DET  |  determiner  |  DT  |  determiner\n",
      "very  |  ADV  |  adverb  |  RB  |  adverb\n",
      "first  |  ADJ  |  adjective  |  JJ  |  adjective (English), other noun-modifier (Chinese)\n",
      "day  |  NOUN  |  noun  |  NN  |  noun, singular or mass\n"
     ]
    }
   ],
   "source": [
    "doc = nlp(\"Wow! Dr. Strange made 265 million $ on the very first day\")\n",
    "\n",
    "for token in doc:\n",
    "    print(token,\" | \", token.pos_, \" | \", spacy.explain(token.pos_), \" | \", token.tag_, \" | \", spacy.explain(token.tag_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "fdb4a11e-fe06-4c1e-807e-e4b676497fa8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "quits | VBZ | verb, 3rd person singular present\n"
     ]
    }
   ],
   "source": [
    "doc = nlp(\"He quits the job\")\n",
    "\n",
    "print(doc[1].text, \"|\", doc[1].tag_, \"|\", spacy.explain(doc[1].tag_))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa546a99-29d3-4738-9305-9db31ec340c5",
   "metadata": {},
   "source": [
    "# In below sentences Spacy figures out the past vs present tense for quit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "22bd718f-3521-483b-8ed0-0821cbf5691e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "quit | VBD | verb, past tense\n"
     ]
    }
   ],
   "source": [
    "doc = nlp(\"he quit the job\")\n",
    "\n",
    "print(doc[1].text, \"|\", doc[1].tag_, \"|\", spacy.explain(doc[1].tag_))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9dab9e6b-481c-44d6-8537-782d8bd63f2e",
   "metadata": {},
   "source": [
    "# Removing all SPACE, PUNCT and X token from text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "89eed441-01ea-4805-949f-1f9857e3533b",
   "metadata": {},
   "outputs": [],
   "source": [
    "earnings_text=\"\"\"Microsoft Corp. today announced the following results for the quarter ended December 31, 2021, as compared to the corresponding period of last fiscal year:\n",
    "\n",
    "·         Revenue was $51.7 billion and increased 20%\n",
    "·         Operating income was $22.2 billion and increased 24%\n",
    "·         Net income was $18.8 billion and increased 21%\n",
    "·         Diluted earnings per share was $2.48 and increased 22%\n",
    "“Digital technology is the most malleable resource at the world’s disposal to overcome constraints and reimagine everyday work and life,” said Satya Nadella, chairman and chief executive officer of Microsoft. “As tech as a percentage of global GDP continues to increase, we are innovating and investing across diverse and growing markets, with a common underlying technology stack and an operating model that reinforces a common strategy, culture, and sense of purpose.”\n",
    "“Solid commercial execution, represented by strong bookings growth driven by long-term Azure commitments, increased Microsoft Cloud revenue to $22.1 billion, up 32% year over year” said Amy Hood, executive vice president and chief financial officer of Microsoft.\"\"\"\n",
    "\n",
    "doc = nlp(earnings_text)\n",
    "\n",
    "filtered_tokens = []\n",
    "\n",
    "for token in doc:\n",
    "    if token.pos_ not in [\"SPACE\", \"PUNCT\", \"X\"]:\n",
    "        filtered_tokens.append(token)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "ec68e4d1-f555-4e32-89e6-15473174ac54",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Microsoft,\n",
       " Corp.,\n",
       " today,\n",
       " announced,\n",
       " the,\n",
       " following,\n",
       " results,\n",
       " for,\n",
       " the,\n",
       " quarter]"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "filtered_tokens[:10]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "8b58f3e2-7d6a-42de-a8c6-124148cf7e47",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{96: 13,\n",
       " 92: 46,\n",
       " 100: 24,\n",
       " 90: 9,\n",
       " 85: 16,\n",
       " 93: 16,\n",
       " 97: 27,\n",
       " 98: 1,\n",
       " 84: 20,\n",
       " 103: 10,\n",
       " 87: 6,\n",
       " 99: 5,\n",
       " 89: 12,\n",
       " 86: 3,\n",
       " 94: 3,\n",
       " 95: 2}"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "count = doc.count_by(spacy.attrs.POS)\n",
    "count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "e5d1f0f9-1fc8-4d38-b8d5-4df22523f473",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'PROPN'"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "doc.vocab[96].text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "197d52d4-068a-4251-bbe3-d91585f13a30",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PROPN | 13\n",
      "NOUN | 46\n",
      "VERB | 24\n",
      "DET | 9\n",
      "ADP | 16\n",
      "NUM | 16\n",
      "PUNCT | 27\n",
      "SCONJ | 1\n",
      "ADJ | 20\n",
      "SPACE | 10\n",
      "AUX | 6\n",
      "SYM | 5\n",
      "CCONJ | 12\n",
      "ADV | 3\n",
      "PART | 3\n",
      "PRON | 2\n"
     ]
    }
   ],
   "source": [
    "for k,v in count.items():\n",
    "    print(doc.vocab[k].text, \"|\",v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1a58dbf-f32b-4ab1-9662-92a801c938bc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "5b4c0ba6-a9e8-484a-a48c-171869069784",
   "metadata": {},
   "source": [
    "<h1 align=\"center\">Named Entity Recognition (NER)</h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "8f86d88b-7893-43b6-8a22-3ff7ec23a630",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(555) 555-5555 PHONE_NUMBER\n"
     ]
    }
   ],
   "source": [
    "#Import the requisite library\n",
    "import spacy\n",
    "\n",
    "#Sample text\n",
    "text = \"This is a sample number (555) 555-5555.\"\n",
    "\n",
    "#Build upon the spaCy Small Model\n",
    "nlp = spacy.blank(\"en\")\n",
    "\n",
    "#Create the Ruler and Add it\n",
    "ruler = nlp.add_pipe(\"entity_ruler\")\n",
    "\n",
    "#List of Entities and Patterns (source: https://spacy.io/usage/rule-based-matching)\n",
    "patterns = [\n",
    "                {\"label\": \"PHONE_NUMBER\", \"pattern\": [{\"ORTH\": \"(\"}, {\"SHAPE\": \"ddd\"}, {\"ORTH\": \")\"}, {\"SHAPE\": \"ddd\"},\n",
    "                {\"ORTH\": \"-\", \"OP\": \"?\"}, {\"SHAPE\": \"dddd\"}]}\n",
    "            ]\n",
    "#add patterns to ruler\n",
    "ruler.add_patterns(patterns)\n",
    "\n",
    "\n",
    "\n",
    "#create the doc\n",
    "doc = nlp(text)\n",
    "\n",
    "#extract entities\n",
    "for ent in doc.ents:\n",
    "    print (ent.text, ent.label_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "dd146064-fe75-4536-8a95-676a89dfe8d9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['tok2vec', 'tagger', 'parser', 'attribute_ruler', 'lemmatizer', 'ner']"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "nlp.pipe_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "f76b83d5-99e2-4408-9362-c885954ce39f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tesla Inc  |  ORG  |  Companies, agencies, institutions, etc.\n",
      "$45 billion  |  MONEY  |  Monetary values, including unit\n"
     ]
    }
   ],
   "source": [
    "doc = nlp(\"Tesla Inc is going to acquire twitter for $45 billion\")\n",
    "for ent in doc.ents:\n",
    "    print(ent.text, \" | \", ent.label_, \" | \", spacy.explain(ent.label_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "24ac6832-a7cf-44a2-9261-81fc68a4ed0a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<span class=\"tex2jax_ignore\"><div class=\"entities\" style=\"line-height: 2.5; direction: ltr\">\n",
       "<mark class=\"entity\" style=\"background: #7aecec; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    Tesla Inc\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">ORG</span>\n",
       "</mark>\n",
       " is going to acquire twitter for \n",
       "<mark class=\"entity\" style=\"background: #e4e7d2; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    $45 billion\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">MONEY</span>\n",
       "</mark>\n",
       "</div></span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from spacy import displacy\n",
    "\n",
    "displacy.render(doc, style=\"ent\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "fe26a60f-0b75-4ce8-9c57-ee679096031a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['CARDINAL',\n",
       " 'DATE',\n",
       " 'EVENT',\n",
       " 'FAC',\n",
       " 'GPE',\n",
       " 'LANGUAGE',\n",
       " 'LAW',\n",
       " 'LOC',\n",
       " 'MONEY',\n",
       " 'NORP',\n",
       " 'ORDINAL',\n",
       " 'ORG',\n",
       " 'PERCENT',\n",
       " 'PERSON',\n",
       " 'PRODUCT',\n",
       " 'QUANTITY',\n",
       " 'TIME',\n",
       " 'WORK_OF_ART']"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nlp.pipe_labels['ner']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "105c4784-3ed0-4db3-b8f2-8ad6f9aba188",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Michael Bloomberg | PERSON | People, including fictional\n",
      "Bloomberg | PERSON | People, including fictional\n",
      "1982 | DATE | Absolute or relative dates or periods\n"
     ]
    }
   ],
   "source": [
    "doc = nlp(\"Michael Bloomberg founded Bloomberg in 1982\")\n",
    "for ent in doc.ents:\n",
    "    print(ent.text, \"|\", ent.label_, \"|\", spacy.explain(ent.label_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "284cbca8-6e77-4cae-8dfb-a6450af90cd9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tesla Inc  |  ORG  |  0 | 9\n",
      "Twitter Inc  |  ORG  |  30 | 41\n",
      "$45 billion  |  MONEY  |  46 | 57\n"
     ]
    }
   ],
   "source": [
    "doc = nlp(\"Tesla Inc is going to acquire Twitter Inc for $45 billion\")\n",
    "for ent in doc.ents:\n",
    "    print(ent.text, \" | \", ent.label_, \" | \", ent.start_char, \"|\", ent.end_char)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "c80aaa15-a093-447b-908d-fe1bca1acd5c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tesla  |  ORG\n",
      "Twitter  |  PRODUCT\n",
      "$45 billion  |  MONEY\n"
     ]
    }
   ],
   "source": [
    "doc = nlp(\"Tesla is going to acquire Twitter for $45 billion\")\n",
    "for ent in doc.ents:\n",
    "    print(ent.text, \" | \", ent.label_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "28189088-3e38-4e63-9f22-69a44452dc52",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "going to acquire"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "s = doc[2:5]\n",
    "s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "8cc8e3d4-7c2a-42d9-9eb4-6d2c1db2e026",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tesla  |  ORG\n",
      "Twitter  |  ORG\n",
      "$45 billion  |  MONEY\n"
     ]
    }
   ],
   "source": [
    "from spacy.tokens import Span\n",
    "\n",
    "s1 = Span(doc, 0, 1, label=\"ORG\")\n",
    "s2 = Span(doc, 5, 6, label=\"ORG\")\n",
    "\n",
    "doc.set_ents([s1, s2], default=\"unmodified\")\n",
    "for ent in doc.ents:\n",
    "    print(ent.text, \" | \", ent.label_)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77b5cb53-b530-47eb-b8d5-f5dff37b9148",
   "metadata": {},
   "source": [
    "# Task1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "8d1100dc-dba0-4d4c-95bd-b386ef0233ca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Geographical location Names: ['India', 'Delhi', 'Gujarat', 'Tamilnadu', 'Pongal', 'Andhrapradesh', 'Assam', 'Bihar']\n",
      "Count: 8\n"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "\n",
    "# Load the pre-trained model for English\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "# Given text\n",
    "text = \"\"\"Kiran want to know the famous foods in each state of India. So, he opened Google and search for this question. \n",
    "Google showed that in Delhi it is Chaat, in Gujarat it is Dal Dhokli, in Tamilnadu it is Pongal, in Andhrapradesh it is Biryani, \n",
    "in Assam it is Papaya Khar, in Bihar it is Litti Chowkha and so on for all other states\"\"\"\n",
    "\n",
    "# Process the text\n",
    "doc = nlp(text)\n",
    "\n",
    "# Extract Geographical Entities\n",
    "geo_names = [ent.text for ent in doc.ents if ent.label_ == \"GPE\"]\n",
    "\n",
    "# Output\n",
    "print(\"Geographical location Names:\", geo_names)\n",
    "print(\"Count:\", len(geo_names))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "7fbd5d6a-17ca-47c2-8e5b-a9b60cfc06b5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['India', 'Delhi', 'Gujarat', 'Tamilnadu', 'Pongal', 'Andhrapradesh', 'Assam', 'Bihar']\n",
      "8\n"
     ]
    }
   ],
   "source": [
    "GPE=[ent.text for ent in doc.ents if ent.label_==\"GPE\"]\n",
    "print(GPE)\n",
    "print(len(GPE))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "131bf594-1d44-4894-a5a1-335d409f4c77",
   "metadata": {},
   "source": [
    "# Task 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "3997a52a-6dd3-446d-9454-1034593b024e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All Birth Dates: ['24 April 1973', '5 November 1988', '7 July 1981', '19 December 1974']\n",
      "Count: 4\n"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "\n",
    "# Load the pre-trained model for English\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "# Given text\n",
    "text = \"\"\"Sachin Tendulkar was born on 24 April 1973, Virat Kholi was born on 5 November 1988, Dhoni was born on 7 July 1981\n",
    "and finally Ricky ponting was born on 19 December 1974.\"\"\"\n",
    "\n",
    "# Process the text\n",
    "doc = nlp(text)\n",
    "\n",
    "# Extract Dates\n",
    "birth_dates = [ent.text for ent in doc.ents if ent.label_ == \"DATE\"]\n",
    "\n",
    "# Output\n",
    "print(\"All Birth Dates:\", birth_dates)\n",
    "print(\"Count:\", len(birth_dates))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "4f4083a1-a1ce-45cc-8013-a8686cd18f4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "\n",
    "# Load the pre-trained model for English\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "# Given text\n",
    "text = \"\"\"Sachin Tendulkar was born on 24 April 1973, Virat Kholi was born on 5 November 1988, Dhoni was born on 7 July 1981\n",
    "and finally Ricky ponting was born on 19 December 1974.\"\"\"\n",
    "\n",
    "# Process the text\n",
    "doc = nlp(text)\n",
    "\n",
    "# Extract Dates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "106f3e65-d426-4263-abea-43ee47ec4a44",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['24 April 1973', '5 November 1988', '7 July 1981', '19 December 1974']\n",
      "Count =  4\n"
     ]
    }
   ],
   "source": [
    "result=[ent.text for ent in doc.ents if ent.label_==\"DATE\"]\n",
    "print(result)\n",
    "print(\"Count = \",len(result) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8f7f2b7-ab3a-4115-807c-128858c6a02e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
